{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 - Taxonomic assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will attempt to assign taxonomic identity to the set of denovo OTUs obtained from the 57 Lake Windermere eDNA samples that we have produced in [exercise-3](https://github.com/HullUni-bioinformatics/metabarcode-course-2016/tree/master/data/exercise-3).\n",
    "\n",
    "We will be using a custom phylogenetically curated reference database (details about how this was created can be found [here](https://github.com/HullUni-bioinformatics/metabarcode-course-2016/tree/master/data/exercise-5/supplementary_data/reference_db)).\n",
    "\n",
    "Taxonomic assignment will be performed using three different approaches:\n",
    " - BLAST based LCA\n",
    " - [pplacer](http://matsen.fhcrc.org/pplacer/) (Phylogenetic placement)\n",
    " - [Kraken](http://ccb.jhu.edu/software/kraken/MANUAL.html) (k-mer based sequence classification)\n",
    "\n",
    "We will again be using our custom pipeline metaBEAT to make our lives easier and to facilitate reproducibility.\n",
    "\n",
    "The final result of this exercise will be taxonomically annotated OTU tables in [BIOM](http://biom-format.org/) format from each approach, which we can then go an compare (if we have time). BIOM format and the associated set of python functions has been developed as a standardized format for representing 'biological sample by observation contingency tables' in the -omics area - so big tables...\n",
    "\n",
    "Most of the input data you will need for the analysis you have already produced in exercise 3. The rest you can find in the folder `input_data`. The directory `supplementary_data` describes the detailed processing steps and all intermediate files to obtain the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Specify the location and file format your reference sequences come in. As before you could mix and match several files with different formats (`fasta`, `Genbank`) if you wanted. You'll need to prepare a simple text file that contains the path to the file and the format specification, like so:\n",
    "\n",
    "```bash\n",
    "PATH/TO/YOUR/sequences1.fasta <tab> fasta\n",
    "PATH/TO/YOUR/sequences2.genbank <tab> gb\n",
    "...\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have prepared a set of reference sequences in Genbank format for you in the directoy `/input_data`. The file is called `CytB_European-fish_SATIVA_cleaned.gb'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time you can use your command line skills to produce the file - We call it `Refmap.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!echo './input_data/CytB_European-fish_SATIVA_cleaned.gb\\tgb' > REFmap.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./input_data/CytB_European-fish_SATIVA_cleaned.gb\tgb\r\n"
     ]
    }
   ],
   "source": [
    "!cat REFmap.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's almost it. Now let's start the pipeline. As input you need to specify the denovo OTU table from exercise 3 in BIOM format and the `REFmap.txt` file. We'll also ask metaBEAT to attempt taxonomic assignment with the three different approaches mentioned above.\n",
    "\n",
    "Phylogenetic placement using `pplacer` requires a number specific files that need to be formatted/prepared in a particular way into a so-called Reference package (aka 'refpkg'). At a minimum this needs to contain a phylogenetic tree, the underlying alignment as `fasta`, a HiddenMarkov model (HMM) profile of the alignemnt, and a number of other files summarizing the taxonomic identity of the taxa in the reference. We are using the program `taxit` from the [taxtastic](http://fhcrc.github.io/taxtastic/commands.html#create) package to do the formatting. What exactly we did to produce the refpkg for this analysis is outlined in [this](https://github.com/HullUni-bioinformatics/metabarcode-course-2016/blob/master/data/exercise-5/supplementary_data/pplacer/build-refpkg/build_pplacer_refpkg.ipynb) notebook.\n",
    "\n",
    "Kraken requires a specific database that metaBEAT will build automatically if necessary, but for the purpose of this course we have prepared it for you (see [here](https://github.com/HullUni-bioinformatics/metabarcode-course-2016/blob/master/data/exercise-5/supplementary_data/kraken/build_custom_db.ipynb) for how it was done) and and it should be present in the `./input_data`.\n",
    "\n",
    "metaBEAT will automatically wrangle the data into the particular file formats that are required by each of the methods, run all necessary steps, and finally convert the outputs of each program to a standardized BIOM table.\n",
    "\n",
    "\n",
    "\n",
    "__Set it off! __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "metaBEAT - metaBarcoding and Environmental DNA Analyses tool\n",
      "version: v.0.96-global\n",
      "\n",
      "\n",
      "Tue Sep 13 09:05:27 2016\n",
      "\n",
      "/usr/bin/metaBEAT_global.py -B ../exercise-3/GLOBAL/CytB-trim30min100-merge-c3-id1-OTU-denovo.biom -R REFmap.txt --blast --min_ident 0.85 --kraken --kraken_db ./input_data/KRAKEN_DB/ --pplace --refpkg ./input_data/CytB.refpkg/ -n 5 -o CytB-trim30min100-merge-c3-id1\n",
      "\n",
      "\n",
      "metaBEAT may be querying NCBI's Entrez databases to fetch/verify taxonomic ids. Entrez User requirements state that you need to identify yourself by providing an email address so that NCBI can contact you in case there is a problem.\n",
      "\n",
      "As the mail address is not specified in the script itself (variable 'Entrez.email'), metaBEAT expects a simple text file called 'user_email.txt' that contains your email address (first line of file) in the same location as the metaBEAT.py script (in your case: /usr/bin/)\n",
      "\n",
      "found 'c.hahn@hull.ac.uk' in /usr/bin/user_email.txt\n",
      "\n",
      "taxonomy.db found at /usr/bin/taxonomy.db\n",
      "\n",
      "parsing BIOM table\n",
      "\n",
      "\n",
      "######## PROCESSING REFERENCE DATA ########\n",
      "\n",
      "\n",
      "processing ./input_data/CytB_European-fish_SATIVA_cleaned.gb (containing 1736 records)\n",
      "\n",
      "total number of valid records: 1736\n",
      "\n",
      "\n",
      "establishing taxonomy for reference sequences from custom database\n",
      "\n",
      "write out taxids to taxids.txt\n",
      "\n",
      "running taxit to generate reduced taxonomy table\n",
      "\n",
      "taxit taxtable -d /usr/bin/taxonomy.db -t taxids.txt -o taxa.csv\n",
      "\n",
      "### BUILDING BLAST DATABASE ###\n",
      "\n",
      "write out reference sequences to refs.fasta\n",
      "\n",
      "makeblastdb -in refs.fasta -dbtype nucl -out marker_blast_db\n",
      "\n",
      "Tue Sep 13 09:05:29 2016\n",
      "\n",
      "\n",
      "denovo OTU table provided - checking if all other files that are required for taxonomic assignment are present\n",
      "\n",
      "expecting to find query sequences ..\n",
      "query sequences detected automatically: /home/working/media/chrishah/STORAGE/Dropbox/Github/metabarcode-course-2016-temp/data/exercise-3/GLOBAL/global_centroids.fasta\n",
      "creating local copies of relevant files if necessary\n",
      "\n",
      "### RUNNING BLAST ###\n",
      "\n",
      "running blast search against database /home/working/media/chrishah/STORAGE/Dropbox/Github/metabarcode-course-2016-temp/data/exercise-5/GLOBAL/BLAST_0.85/marker_blast_db\n",
      "blastn -query /home/working/media/chrishah/STORAGE/Dropbox/Github/metabarcode-course-2016-temp/data/exercise-3/GLOBAL/global_centroids.fasta -db /home/working/media/chrishah/STORAGE/Dropbox/Github/metabarcode-course-2016-temp/data/exercise-5/GLOBAL/BLAST_0.85/marker_blast_db -evalue 1e-20 -outfmt 5 -out global_blastn.out.xml -num_threads 5 -max_target_seqs 50\n",
      "\n",
      "Tue Sep 13 09:06:09 2016\n",
      "\n",
      "\n",
      "### INTERPRETING BLAST RESULTS ###\n",
      "\n",
      "\n",
      "parse blast xml file\n",
      "\n",
      "get top 10% hits from blast output ..  6712 queries processed\n",
      "\n",
      "assign taxonomy\n",
      "\n",
      "all queries have been successfully assigned to a taxonomy\n",
      "##### FORMATTING AND WRITING BIOM OUTPUT #####\n",
      "\n",
      "\n",
      "##### BLAST ANALYSIS DONE #####\n",
      "\n",
      "Find result tables in '/home/working/media/chrishah/STORAGE/Dropbox/Github/metabarcode-course-2016-temp/data/exercise-5/GLOBAL/BLAST_0.85'\n",
      "\n",
      "\n",
      "{'OTU_denovo': 6712 x 57 <class 'biom.table.Table'> with 13648 nonzero entries (3% dense), 'blast': {'OTU_taxonomy': 6712 x 57 <class 'biom.table.Table'> with 13648 nonzero entries (3% dense), 'cluster_taxonomy': 19 x 57 <class 'biom.table.Table'> with 304 nonzero entries (28% dense), 'taxon_taxonomy': 19 x 57 <class 'biom.table.Table'> with 304 nonzero entries (28% dense)}}\n",
      "\n",
      "Tue Sep 13 09:06:43 2016\n",
      "\n",
      "\n",
      "##### PHYLOGENETIC PLACEMENT WITH PPLACER #####\n",
      "\n",
      "\n",
      "establishing taxonomy for reference sequences from custom database\n",
      "\n",
      "write out taxids to taxids.txt\n",
      "\n",
      "running taxit to generate reduced taxonomy table\n",
      "\n",
      "taxit taxtable -d /usr/bin/taxonomy.db -t taxids.txt -o taxa.csv\n",
      "\n",
      "### PREPARING PPLACER INPUT ###\n",
      "\n",
      "pplacer analysis will be limited to queries with at least 85.0 % similarity to the reference database\n",
      "\n",
      "This will be determined based on a blast analysis ..\n",
      "Filtering queries for pplacer based on results from previous BLAST result : /home/working/media/chrishah/STORAGE/Dropbox/Github/metabarcode-course-2016-temp/data/exercise-5/GLOBAL/BLAST_0.85/global_blastn.out.xml\n",
      "parse blast xml file\n",
      "pre-bin based on blast output ..   number of queries processed: 6712\n",
      "\n",
      "Number of queries passed to pplacer:\t1794\n",
      "(0 will be reverse complemented first)\n",
      "Number of queries directly binned to 'unassigned':\t4918\n",
      "\n",
      "parsing pplacer refpkg: /home/working/media/chrishah/STORAGE/Dropbox/Github/metabarcode-course-2016-temp/data/exercise-5/input_data/CytB.refpkg\n",
      "\n",
      "found hmm profile: /home/working/media/chrishah/STORAGE/Dropbox/Github/metabarcode-course-2016-temp/data/exercise-5/input_data/CytB.refpkg/CytB_ref.hmm\n",
      "found alignment: /home/working/media/chrishah/STORAGE/Dropbox/Github/metabarcode-course-2016-temp/data/exercise-5/input_data/CytB.refpkg/CytB_European-fish_SATIVA_cleaned.alignment.fasta\n",
      "\n",
      "### ALIGN QUERIES TO HMM PROFILE ###\n",
      "\n",
      "hmmalign -o queries_to_profile.sto --mapali /home/working/media/chrishah/STORAGE/Dropbox/Github/metabarcode-course-2016-temp/data/exercise-5/input_data/CytB.refpkg/CytB_European-fish_SATIVA_cleaned.alignment.fasta /home/working/media/chrishah/STORAGE/Dropbox/Github/metabarcode-course-2016-temp/data/exercise-5/input_data/CytB.refpkg/CytB_ref.hmm /home/working/media/chrishah/STORAGE/Dropbox/Github/metabarcode-course-2016-temp/data/exercise-5/GLOBAL/PPLACER/pplacer.queries.fasta\n",
      "\n",
      "\n",
      "### RUNNING PPLACER ###\n",
      "\n",
      "pplacer -c /home/working/media/chrishah/STORAGE/Dropbox/Github/metabarcode-course-2016-temp/data/exercise-5/input_data/CytB.refpkg queries_to_profile.sto -p --keep-at-most 3 -o pplacer.jplace\n",
      "\n",
      "\n",
      "### INTERPRETING PPLACER RESULTS ###\n",
      "\n",
      "##### FORMATTING AND WRITING BIOM OUTPUT #####\n",
      "\n",
      "{'OTU_denovo': 6712 x 57 <class 'biom.table.Table'> with 13648 nonzero entries (3% dense), 'blast': {'OTU_taxonomy': 6712 x 57 <class 'biom.table.Table'> with 13648 nonzero entries (3% dense), 'cluster_taxonomy': 19 x 57 <class 'biom.table.Table'> with 304 nonzero entries (28% dense), 'taxon_taxonomy': 19 x 57 <class 'biom.table.Table'> with 304 nonzero entries (28% dense)}, 'pplacer': {'OTU_taxonomy': 6712 x 57 <class 'biom.table.Table'> with 13648 nonzero entries (3% dense), 'cluster_taxonomy': 20 x 57 <class 'biom.table.Table'> with 293 nonzero entries (25% dense), 'taxon_taxonomy': 20 x 57 <class 'biom.table.Table'> with 293 nonzero entries (25% dense)}}\n",
      "\n",
      "##### PPLACER ANALYSIS DONE #####\n",
      "\n",
      "Find result tables in '/home/working/media/chrishah/STORAGE/Dropbox/Github/metabarcode-course-2016-temp/data/exercise-5/GLOBAL/PPLACER'\n",
      "\n",
      "\n",
      "\n",
      "Tue Sep 13 09:12:12 2016\n",
      "\n",
      "\n",
      "##### RUNNING KRAKEN #####\n",
      "\n",
      "\n",
      "establishing taxonomy for reference sequences from custom database\n",
      "\n",
      "write out taxids to taxids.txt\n",
      "\n",
      "running taxit to generate reduced taxonomy table\n",
      "\n",
      "taxit taxtable -d /usr/bin/taxonomy.db -t taxids.txt -o taxa.csv\n",
      "\n",
      "Using precompiled kraken database: /home/working/media/chrishah/STORAGE/Dropbox/Github/metabarcode-course-2016-temp/data/exercise-5/input_data/KRAKEN_DB\n",
      "\n",
      "## Running Kraken ##\n",
      "\n",
      "kraken --threads 5 --db /home/working/media/chrishah/STORAGE/Dropbox/Github/metabarcode-course-2016-temp/data/exercise-5/input_data/KRAKEN_DB /home/working/media/chrishah/STORAGE/Dropbox/Github/metabarcode-course-2016-temp/data/exercise-3/GLOBAL/global_centroids.fasta\n",
      "\n",
      "\r",
      "Processed 2185 sequences (351816 bp) ...\r",
      "Processed 3395 sequences (851929 bp) ...\r",
      "Processed 4601 sequences (1352139 bp) ...\r",
      "Processed 6712 sequences (1852294 bp) ...\r",
      "6712 sequences (1.85 Mbp) processed in 0.135s (2976.1 Kseq/m, 821.32 Mbp/m).\n",
      "  2650 sequences classified (39.48%)\n",
      "  4062 sequences unclassified (60.52%)\n",
      "\n",
      "##### FORMATTING AND WRITING BIOM OUTPUT #####\n",
      "\n",
      "{'OTU_denovo': 6712 x 57 <class 'biom.table.Table'> with 13648 nonzero entries (3% dense), 'blast': {'OTU_taxonomy': 6712 x 57 <class 'biom.table.Table'> with 13648 nonzero entries (3% dense), 'cluster_taxonomy': 19 x 57 <class 'biom.table.Table'> with 304 nonzero entries (28% dense), 'taxon_taxonomy': 19 x 57 <class 'biom.table.Table'> with 304 nonzero entries (28% dense)}, 'kraken': {'OTU_taxonomy': 6712 x 57 <class 'biom.table.Table'> with 13648 nonzero entries (3% dense), 'cluster_taxonomy': 20 x 57 <class 'biom.table.Table'> with 321 nonzero entries (28% dense), 'taxon_taxonomy': 20 x 57 <class 'biom.table.Table'> with 321 nonzero entries (28% dense)}, 'pplacer': {'OTU_taxonomy': 6712 x 57 <class 'biom.table.Table'> with 13648 nonzero entries (3% dense), 'cluster_taxonomy': 20 x 57 <class 'biom.table.Table'> with 293 nonzero entries (25% dense), 'taxon_taxonomy': 20 x 57 <class 'biom.table.Table'> with 293 nonzero entries (25% dense)}}\n",
      "\n",
      "##### KRAKEN ANALYSIS DONE #####\n",
      "\n",
      "Find result tables in '/home/working/media/chrishah/STORAGE/Dropbox/Github/metabarcode-course-2016-temp/data/exercise-5/GLOBAL/KRAKEN'\n",
      "\n",
      "\n",
      "\n",
      "Tue Sep 13 09:12:21 2016\n",
      "\n",
      "\n",
      "##### DONE! #####\n",
      "\n",
      "\n",
      "Tue Sep 13 09:12:21 2016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "metaBEAT_global.py \\\n",
    "-B ../exercise-3/GLOBAL/CytB-trim30min100-merge-c3-id1-OTU-denovo.biom \\\n",
    "-R REFmap.txt \\\n",
    "--blast --min_ident 0.85 \\\n",
    "--kraken --kraken_db ./input_data/KRAKEN_DB/ \\\n",
    "--pplace --refpkg ./input_data/CytB.refpkg/ \\\n",
    "-n 5 -o CytB-trim30min100-merge-c3-id1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Detailed explanation of the above command:\n",
    "```bash\n",
    "metaBEAT_global.py \\\n",
    "-B ../exercise-3/GLOBAL/CytB-trim30min100-merge-c3-id1-OTU-denovo.biom \\ #denovo BIOM table\n",
    "-R REFmap.txt \\ #file listing all sequence files to be used as reference\n",
    "--blast \\ #specify BLAST LCA based assignment strategy\n",
    "--min_ident 0.85 \\ #only attempt assignment for queries with at lest 85% identity to any reference sequence (relevant only for BLAST and pplacer)\n",
    "--kraken  \\ #specify to use Kraken for assignment\n",
    "--kraken_db ./input_data/KRAKEN_DB/ \\ #path to the Kraken database (if ommitted metabeat will generate it)\n",
    "--pplace \\ #specify to use pplacer for assignment\n",
    "--refpkg ./input_data/CytB.refpkg/ \\ #Refpackage for pplacer\n",
    "-n 5 \\ #use 5 threads\n",
    "-o CytB-trim30min100-merge-c3-id1 &> taxonomic_assignment.log #prefix for output and write log to file\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the entire process only took ~10 minutes.\n",
    "\n",
    "As before, everything that metaBEAT did is presented to you in the output (see above) and all intermediate files are kept in the directory `./GLOBAL`, and for each of the three approaches in a separate directory within.\n",
    "\n",
    "Scroll through the output cell to get a quick idea of all the things that the pipeline just did for you. It's not needed to go into detail, just have a quick look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metaBEAT has produced results tables in BIOM format for each of the approaches in the corresponding directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice tool for interactive exploration of your results is [phinch](http://phinch.org/). Load one of the tables (`*.biom`) and see if you can get an overview of your results.\n",
    "\n",
    "As mentioned before, the BIOM format is a standardized file format and there is a range of tools out there that can be used to analyse these data.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
