{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Lake Windermere eNDA metabarcoding data \n",
    "\n",
    "In this exercise we will use [metaBEAT](https://github.com/HullUni-bioinformatics/metaBEAT), a tool tailored towards reproducible and efficient analyses of metabarcoding data that we have developed in-house. It is still under active development and will likely be extended further in the future. The pipeline is available in a Docker [container](https://registry.hub.docker.com/u/chrishah/metabeat/) with all necessary dependencies. The Docker image is building on [ReproPhylo](https://registry.hub.docker.com/u/szitenberg/reprophylo/).\n",
    "\n",
    "The data we will be analyzing are CytB sequences amplified from eDNA samples collected from Lake Windermere. The experiment was designed to assess the potential of the eDNA approach to assess fish community compositions and was published recently (Haenfling et al. 2016). \n",
    "The metaBEAT tool is designed as a wrapper around a complete analysis from raw data, performing (optionally) de-multiplexing, quality filtering, clustering along the way, to OTU tables in biom format. It currently supports BLAST and phylogenetic placement (pplacer). The plan is to include further approaches in the future and to allow for efficient and standardized comparative assessments of all approaches. \n",
    "\n",
    "metBEAT offers a large number of options. Most of them will sound familiar and should make sense to you given your experience from the course so far. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!metaBEAT_global.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will limit ourselves to a basic processing (read trimming, -merging-, clustering) for now.\n",
    "\n",
    "Minimum input for an anlysis is a set of query sequences in one or several files (accepted are a number of file formats, e.g. `fasta`, `fastq`). These will be run through the pipeline sequentially.\n",
    "\n",
    "You will need to provide information on the nature and location of the query sequence files in a separate tab-delimited text files via the `-Q` flags,\n",
    "\n",
    "The format for this text file is as follows:\n",
    "\n",
    "`unique sample_ID <tab> format <tab> file1 <tab> file2 (optional) <tab> optionally barcodes`\n",
    "\n",
    "You may generate the required text files in any text editor. But for the sake of reproducibility, let's use a mini python script to programatically produce this file, which I will call `Querymap.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "out_list = []\n",
    "string = ''\n",
    "datadir = '../raw_Illumina_data/reads/'\n",
    "files = os.listdir(datadir)\n",
    "for f in sorted(files):\n",
    "    if '_1.fastq' in f:\n",
    "        string += \"%s\\tfastq\\t%s%s\\t%s/%s\" %(f.replace('-CytB_1.fastq.gz', ''), datadir, f, datadir, f.replace('_1.fastq', '_2.fastq'))+'\\n'\n",
    "\n",
    "out = open('Querymap.txt', 'w')\n",
    "out.write(string)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look (note that the output is probably line-wrapped). Another example can be found [here](https://github.com/HullUni-bioinformatics/metabarcode-course-2016/blob/master/data/exercise-3/example_data/Querymap.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!head Querymap.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo -e \"Starttime: $(date)\\n\"\n",
    "\n",
    "metaBEAT_global.py \\\n",
    "-Q Querymap.txt \\\n",
    "--trim_qual 30 \\\n",
    "--trim_minlength 100 \\\n",
    "--merge \\\n",
    "--forward_only \\\n",
    "--product_length 400 \\\n",
    "--cluster --clust_match 1 --clust_cov 3 \\\n",
    "-m CytB -o CytB-trim30min100-merge-c3-id1 \\\n",
    "-n 5 -v &> log\n",
    "\n",
    "echo -e \"Endtime: $(date)\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read processing will take about 20-30 minutes.\n",
    "\n",
    "Some more details about the above analysis:\n",
    "\n",
    "```bash\n",
    "metaBEAT_global.py \\ #call the program\n",
    "-Q Querymap.txt \\ #path to the Querymap\n",
    "--trim_qual 30 \\ #minimum quality for trimming is prhed Q30 (trimmomatic)\n",
    "--trim_minlength 100 \\ #only retain reads longer than 100 bp after trimming\n",
    "--merge \\ #merge overlapping read pairs (flash)\n",
    "--forward_only \\ #retain only forward read in cases where read pairs don't merge\n",
    "--product_length 400 \\ #expected length of merged read is roughly 400 bp\n",
    "--cluster \\ #cluster sequences (vsearch)\n",
    "--clust_match 1 \\ #cluster at 100 % identity\n",
    "--clust_cov 3 \\ #minimum coverage of 3 reads to retain cluster\n",
    "-m CytB \\ #arbitrary name of the marker\n",
    "-o CytB-trim30min100-merge-c3-id1 \\ #prefix for results files\n",
    "-n 5 \\ #use 5 threads\n",
    "-v &> log #write verbose output to file called log\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the read processing is running we can continue with the course program. Next we will focus on constructing/curating a custom reference database.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " metaBEAT will generate a directory with all temporary files that were create during the processing for each sample and will record useful stats summarizing the data processing in the file `CytB-trim30min100-merge-c3-id1_read_stats.csv`.\n",
    " \n",
    "You can explore the table or we can quickly plot out some of these stats here in the notebook, e.g. like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# parse metaBEAT_read_stats.csv\n",
    "stats_file = 'CytB-trim30min100-merge-c3-id1_read_stats.csv'\n",
    "\n",
    "################\n",
    "## prepare data\n",
    "total_pairs = []\n",
    "trimmed_pairs = []\n",
    "trimmed_orphams = []\n",
    "merged_pairs = []\n",
    "queries = []\n",
    "\n",
    "for l in open(stats_file, 'r').readlines()[1:]:\n",
    "    smpl, tot, trimtot, trimpe, trimorph, merged, n1,n2,n3,n4, q = l.rstrip().split(',')\n",
    "    total_pairs.append(int(tot)/2.0)\n",
    "    trimmed_pairs.append(int(trimpe)/2.0)\n",
    "    trimmed_orphams.append(int(trimorph))\n",
    "    merged_pairs.append(int(merged))\n",
    "    queries.append(int(q))\n",
    "    \n",
    "dat = [total_pairs, trimmed_pairs, trimmed_orphams, merged_pairs, queries]\n",
    "dat = [np.array(d) for d in dat]\n",
    "x = ['raw pairs', 'trimmed pairs', 'trimmed orphans', 'merged pairs', 'queries']\n",
    "\n",
    "################\n",
    "## do the plotting\n",
    "fig = plt.figure(1, figsize=(12, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "bp = ax.boxplot(dat)\n",
    "bp = ax.set_xticklabels(x, rotation=45)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detailed info on the entire process you can find in the log file `log`. It contains the exact commands that were run for each sample during each step of the process.\n",
    "\n",
    "It's a large text file - just have a look at the first 100 lines for a start. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -n 100 log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__WELL DONE!__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
